#!/bin/bash

# üåê Script para Executar Crawler via API em Produ√ß√£o
# 
# Uso:
#   ./scripts/api-crawler.sh [op√ß√µes]
#
# Configura√ß√£o:
#   Crie um arquivo .env.crawler com:
#   API_URL=https://gamescatalog.net
#   API_USER=admin
#   API_PASS=sua_senha

set -e

# Cores
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Configura√ß√µes padr√£o
DRY_RUN=false
MAX_GAMES=5
ENV_FILE=".env.crawler"

# Carregar configura√ß√µes do arquivo .env.crawler
if [ -f "$ENV_FILE" ]; then
    source "$ENV_FILE"
else
    echo -e "${RED}‚ùå Arquivo $ENV_FILE n√£o encontrado!${NC}"
    echo -e "${YELLOW}üìù Crie o arquivo com:${NC}"
    echo "API_URL=https://gamescatalog.net"
    echo "API_USER=admin"
    echo "API_PASS=sua_senha"
    exit 1
fi

# Verificar vari√°veis obrigat√≥rias
if [ -z "$API_URL" ] || [ -z "$API_USER" ] || [ -z "$API_PASS" ]; then
    echo -e "${RED}‚ùå Vari√°veis obrigat√≥rias n√£o configuradas no $ENV_FILE${NC}"
    exit 1
fi

# Processar argumentos
while [[ $# -gt 0 ]]; do
    case $1 in
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        --max-games)
            MAX_GAMES="$2"
            shift 2
            ;;
        --help)
            echo -e "${BLUE}üåê Crawler via API - Produ√ß√£o${NC}"
            echo ""
            echo "Uso: ./scripts/api-crawler.sh [op√ß√µes]"
            echo ""
            echo "Op√ß√µes:"
            echo "  --dry-run       Simula execu√ß√£o"
            echo "  --max-games N   M√°ximo de jogos (padr√£o: 5)"
            echo "  --help          Esta ajuda"
            exit 0
            ;;
        *)
            echo -e "${RED}‚ùå Op√ß√£o desconhecida: $1${NC}"
            exit 1
            ;;
    esac
done

# Fun√ß√£o para fazer requisi√ß√£o
api_request() {
    local method="$1"
    local endpoint="$2"
    local data="$3"
    
    local auth=$(echo -n "$API_USER:$API_PASS" | base64)
    
    if [ -n "$data" ]; then
        curl -s -X "$method" \
             -H "Content-Type: application/json" \
             -H "Authorization: Basic $auth" \
             -d "$data" \
             "$API_URL/api/metacritic$endpoint"
    else
        curl -s -X "$method" \
             -H "Authorization: Basic $auth" \
             "$API_URL/api/metacritic$endpoint"
    fi
}

echo -e "${BLUE}"
echo "=========================================="
echo "üåê Crawler via API - Produ√ß√£o"
echo "=========================================="
echo -e "${NC}"

echo -e "${YELLOW}üìä Configura√ß√£o:${NC}"
echo "   URL: $API_URL"
echo "   Usu√°rio: $API_USER"
echo "   Modo: $([ "$DRY_RUN" = true ] && echo "DRY RUN" || echo "REAL")"
echo "   M√°ximo de jogos: $MAX_GAMES"
echo ""

# 1. Verificar status
echo -e "${BLUE}üîç Verificando status...${NC}"
status_response=$(api_request "GET" "/status")

if echo "$status_response" | jq -e '.success' > /dev/null 2>&1; then
    games_count=$(echo "$status_response" | jq -r '.data.gamesWithoutScores')
    echo -e "${GREEN}‚úÖ Jogos sem nota Metacritic: $games_count${NC}"
    
    if [ "$games_count" -eq 0 ]; then
        echo -e "${GREEN}üéâ Todos os jogos j√° possuem notas!${NC}"
        exit 0
    fi
else
    echo -e "${RED}‚ùå Erro ao verificar status${NC}"
    echo "$status_response"
    exit 1
fi

# 2. Confirma√ß√£o
if [ "$DRY_RUN" = false ]; then
    echo -e "${YELLOW}‚ö†Ô∏è  ATEN√á√ÉO: Ir√° alterar o banco de produ√ß√£o!${NC}"
    read -p "Continuar? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo -e "${BLUE}üìù Cancelado${NC}"
        exit 0
    fi
fi

# 3. Executar crawler
echo -e "${BLUE}üöÄ Executando crawler...${NC}"

crawl_data=$(cat <<EOF
{
    "maxGames": $MAX_GAMES,
    "dryRun": $DRY_RUN
}
EOF
)

crawl_response=$(api_request "POST" "/crawl" "$crawl_data")

if echo "$crawl_response" | jq -e '.success' > /dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Crawler executado com sucesso!${NC}"
    echo ""
    
    # Mostrar resultados
    processed=$(echo "$crawl_response" | jq -r '.data.processed')
    updated=$(echo "$crawl_response" | jq -r '.data.updated')
    failed=$(echo "$crawl_response" | jq -r '.data.failed')
    success_rate=$(echo "$crawl_response" | jq -r '.data.successRate')
    
    echo -e "${YELLOW}üìä Resultados:${NC}"
    echo "   Processados: $processed"
    echo "   Atualizados: $updated"
    echo "   Falharam: $failed"
    echo "   Taxa de sucesso: $success_rate%"
    
    # Mostrar erros se houver
    errors=$(echo "$crawl_response" | jq -r '.data.errors[]?' 2>/dev/null || echo "")
    if [ -n "$errors" ]; then
        echo ""
        echo -e "${YELLOW}‚ö†Ô∏è  Erros:${NC}"
        echo "$crawl_response" | jq -r '.data.errors[]?' | while read -r error; do
            echo "   - $error"
        done
    fi
    
else
    echo -e "${RED}‚ùå Erro ao executar crawler${NC}"
    echo "$crawl_response"
    exit 1
fi

echo ""
echo -e "${BLUE}"
echo "=========================================="
echo "üéØ Finalizado!"
echo "=========================================="
echo -e "${NC}" 